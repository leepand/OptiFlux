import argparse
import logging
import yaml
import os
from pathlib import Path
from .utils.file_utils import data_dir_default
import hashlib
import json
import requests
from datetime import datetime
import zipfile
import io

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

log_base_path = os.path.join(data_dir_default(), "logs")

DEFAULT_ENV_TEMPLATE = f"""# OptiFlux ç¯å¢ƒé…ç½®
# æœåŠ¡å™¨åŸºç¡€é…ç½®
SERVER_HOST=0.0.0.0
SERVER_PORT=8912

# æ¨¡å‹ç›®å½•é…ç½®
DEV_ENV_DIR=./models/dev
PREPROD_ENV_DIR=./models/preprod
PROD_ENV_DIR=./models/prod

# æ—¥å¿—é…ç½®
LOG_DIR={log_base_path}

# Flask å¼€å‘æ¨¡å¼é…ç½®
FLASK_DEBUG=true

# Gunicorn ç”Ÿäº§æ¨¡å¼é…ç½®
GUNICORN_WORKERS=4
GUNICORN_TIMEOUT=30
GUNICORN_LOGLEVEL=info
"""

# SERVER_URL = "http://35.165.37.114:8913"

IGNORE_PATTERNS = [
    ".ipynb_checkpoints",  # å¿½ç•¥ Jupyter Notebook çš„æ£€æŸ¥ç‚¹ç›®å½•
    ".optiflux/index",     # å¿½ç•¥ç´¢å¼•æ–‡ä»¶
    ".optiflux",
    "servers.yaml"
]

class OptifluxClient:
    def __init__(self, repo_path, server_name=None):
        self.repo_path = repo_path
        self.git_dir = os.path.join(repo_path, ".optiflux")
        self.objects_dir = os.path.join(self.git_dir, "objects")
        self.head_path = os.path.join(self.git_dir, "HEAD")
        self.server_name = server_name
        self.server_info = self.load_server_info()
        self.session = requests.Session()
        os.makedirs(self.git_dir, exist_ok=True)
        os.makedirs(self.objects_dir, exist_ok=True)

    def load_session(self, server_name):
        """åŠ è½½æŒ‡å®šæœåŠ¡å™¨çš„ä¼šè¯ä¿¡æ¯"""
        session_path = os.path.join(self.git_dir, f"session_{server_name}")
        if os.path.exists(session_path):
            with open(session_path, "r") as f:
                session_data = json.load(f)
                self.session.cookies.update(session_data.get("cookies", {}))
                self.user_id = session_data.get("user_id")
        else:
            self.user_id = None

    def save_session(self, server_name):
        """ä¿å­˜æŒ‡å®šæœåŠ¡å™¨çš„ä¼šè¯ä¿¡æ¯"""
        session_path = os.path.join(self.git_dir, f"session_{server_name}")
        session_data = {
            "cookies": dict(self.session.cookies),
            "user_id": self.user_id,
        }
        with open(session_path, "w") as f:
            json.dump(session_data, f)

    def login(self, username, password, server_name):
        """ç™»å½•æŒ‡å®šæœåŠ¡å™¨å¹¶ä¿å­˜ä¼šè¯ä¿¡æ¯"""
        url = f"{self.get_server_url()}/login"
        try:
            response = self.session.post(
                url,
                data={"username": username, "password": password},
                headers={"Accept": "application/json"}  # æŒ‡å®šéœ€è¦ JSON å“åº”
            )
            print(f"Response status code: {response.status_code}")
            print(f"Response content: {response.text}")
            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get("status") == "success":
                        self.user_id = data.get("user_id")
                        self.save_session(server_name)
                        print(f"Login to {server_name} successful")
                    else:
                        print(f"Login failed: {data.get('message')}")
                except ValueError as e:
                    print(f"Failed to parse response as JSON: {e}")
            else:
                print(f"Login to {server_name} failed")
                self.user_id = None
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")


    def login2(self, username, password, server_name):
        """ç™»å½•æŒ‡å®šæœåŠ¡å™¨å¹¶ä¿å­˜ä¼šè¯ä¿¡æ¯"""
        url = f"{self.get_server_url()}/login"
        response = self.session.post(
            url, data={"username": username, "password": password}
        )
        if response.status_code == 200:
            self.user_id = response.json().get("_user_id")
            self.save_session(server_name)
            print(f"Login to {server_name} successful")
        else:
            print(f"Login to {server_name} failed")
            self.user_id = None

    def load_server_info(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½æœåŠ¡å™¨ä¿¡æ¯"""
        config_path = os.path.join(self.repo_path, "servers.yaml")
        if not os.path.exists(config_path):
            raise FileNotFoundError(
                f"Server configuration file not found: {config_path}"
            )

        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        if self.server_name:  # and self.server_name in config['servers']:
            return config["servers"][self.server_name]
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæœåŠ¡å™¨åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªæœåŠ¡å™¨
            return list(config["servers"].values())[0]

    def get_server_url(self):
        """è·å–å½“å‰æœåŠ¡å™¨çš„URL"""
        return self.server_info["url"]

    def get_api_key(self):
        """è·å–å½“å‰æœåŠ¡å™¨çš„API Key"""
        return self.server_info["api_key"]

    def get_index_path(self):
        return os.path.join(self.git_dir, "index")

    def read_index(self):
        """è¯»å–ç´¢å¼•æ–‡ä»¶"""
        index_path = self.get_index_path()
        if os.path.exists(index_path):
            with open(index_path, "r") as f:
                return json.load(f)
        return {}

    def write_index(self, index):
        """å†™å…¥ç´¢å¼•æ–‡ä»¶"""
        index_path = self.get_index_path()
        with open(index_path, "w") as f:
            json.dump(index, f)

    def hash_object2(self, data):
        """ç”Ÿæˆå¯¹è±¡çš„å“ˆå¸Œå€¼"""
        return hashlib.sha1(data.encode()).hexdigest()

    def hash_object(self, data):
        """ç”Ÿæˆå¯¹è±¡çš„å“ˆå¸Œå€¼"""
        if isinstance(data, str):  # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆç¼–ç ä¸º bytes
            data = data.encode("utf-8")
        return hashlib.sha1(data).hexdigest()  # ç›´æ¥è®¡ç®—å“ˆå¸Œå€¼

    def add(self, path):
        """æ·»åŠ æ–‡ä»¶åˆ°æš‚å­˜åŒº"""
        index = self.read_index()
        index["operations"] = []

        def should_ignore(file_path):
            """æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ¹é…å¿½ç•¥è§„åˆ™"""
            for pattern in IGNORE_PATTERNS:
                if pattern in file_path:
                    return True
            return False

        if os.path.isdir(path):
            for root, _, files in os.walk(path):
                for file in files:
                    file_path = os.path.join(root, file)
                    if should_ignore(file_path):
                        print(f"Ignored {file_path}")
                        continue
                    try:
                        with open(file_path, "rb") as f:  # ä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
                            content = f.read()
                        file_hash = self.hash_object(content)

                        if file_path not in index:
                            # æ–‡ä»¶æ–°å¢
                            index[file_path] = file_hash
                            index["operations"].append(
                                {
                                    "type": "add",
                                    "file": file_path,
                                    "old_hash": file_hash,
                                    "new_hash": file_hash,
                                }
                            )
                            print(f"Added {file_path}")
                        elif index[file_path] != file_hash:
                            # æ–‡ä»¶æ›´æ–°
                            index["operations"].append(
                                {
                                    "type": "update",
                                    "file": file_path,
                                    "old_hash": index[file_path],
                                    "new_hash": file_hash,
                                }
                            )

                            print(f"Updated {file_path}")

                    except UnicodeDecodeError:
                        print(f"Skipped binary file: {file_path}")
        else:
            if should_ignore(path):
                print(f"Ignored {path}")
                return
            try:
                with open(path, "rb") as f:  # ä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
                    content = f.read()
                file_hash = self.hash_object(content)

                if path not in index:
                    # æ–‡ä»¶æ–°å¢
                    index["operations"].append(
                        {
                            "type": "add",
                            "file": path,
                            "old_hash": file_hash,
                            "new_hash": file_hash,
                        }
                    )
                    print(f"Added {path}")
                elif index[path] != file_hash:
                    # æ–‡ä»¶æ›´æ–°
                    index["operations"].append(
                        {
                            "type": "update",
                            "file": path,
                            "old_hash": index[path],
                            "new_hash": file_hash,
                        }
                    )

                    print(f"Updated {path}")

            except UnicodeDecodeError:
                print(f"Skipped binary file: {path}")
        self.write_index(index)

    def commit(self, message):
        """æäº¤æ›´æ”¹åˆ°æœåŠ¡ç«¯"""
        commit_data = {
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "files": {},
            "operations": [],  # è®°å½•æ“ä½œä¿¡æ¯
        }

        def should_ignore(file_path):
            """æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ¹é…å¿½ç•¥è§„åˆ™"""
            for pattern in IGNORE_PATTERNS:
                if pattern in file_path:
                    return True
            return False

        index = self.read_index()
        operations = index.get("operations", [])
        commit_data["operations"] = operations
        for file_path, file_hash in list(
            index.items()
        ):  # ä½¿ç”¨ list() é¿å…è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            if should_ignore(file_path):
                print(f"Ignored {file_path}")
                continue
            if file_path == "operations":  # è·³è¿‡æ“ä½œä¿¡æ¯
                continue
            if os.path.exists(file_path):  # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                with open(file_path, "rb") as f:
                    content = f.read()
                current_hash = self.hash_object(content)

                if file_hash != current_hash:
                    # æ–‡ä»¶å†…å®¹å·²æ›´æ–°
                    commit_data["files"][file_path] = current_hash
                    commit_data["operations"].append(
                        {
                            "type": "update",
                            "file": file_path,
                            "old_hash": file_hash,
                            "new_hash": current_hash,
                        }
                    )
                    index[file_path] = current_hash  # æ›´æ–°ç´¢å¼•ä¸­çš„å“ˆå¸Œå€¼
                    print(f"Updated {file_path}")
                else:
                    # æ–‡ä»¶æœªæ›´æ”¹
                    commit_data["files"][file_path] = file_hash
            else:
                # æ–‡ä»¶å·²åˆ é™¤
                commit_data["operations"].append(
                    {
                        "type": "delete",
                        "file": file_path,
                        "old_hash": file_hash,
                    }
                )
                del index[file_path]  # ä»ç´¢å¼•ä¸­ç§»é™¤
                print(f"Removed non-existent file from index: {file_path}")

        # operations_info = commit_data["operations"]
        index["operations"] = commit_data["operations"]
        # æ›´æ–°ç´¢å¼•æ–‡ä»¶
        self.write_index(index)

        # å‘é€æäº¤æ•°æ®åˆ°æœåŠ¡ç«¯
        response = requests.post(
            f"{self.get_server_url()}/commit", json={"commit": commit_data}
        )
        if response.status_code == 200:
            commit_hash = response.json()["commit_hash"]
            print(f"Commit {commit_hash} created")
        else:
            print("Failed to create commit")

    def commit2(self, message):
        """æäº¤æ›´æ”¹åˆ°æœåŠ¡ç«¯"""
        commit_data = {
            "message": message,
            "timestamp": datetime.now().isoformat(),
            "files": {},
        }

        index = self.read_index()
        for file_path, file_hash in list(
            index.items()
        ):  # ä½¿ç”¨ list() é¿å…è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            if os.path.exists(file_path):  # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                with open(file_path, "rb") as f:
                    content = f.read()
                commit_data["files"][file_path] = file_hash
            else:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»ç´¢å¼•ä¸­ç§»é™¤
                del index[file_path]
                print(f"Removed non-existent file from index: {file_path}")

        # æ›´æ–°ç´¢å¼•æ–‡ä»¶
        self.write_index(index)

        response = requests.post(
            f"{self.get_server_url()}/commit", json={"commit": commit_data}
        )
        if response.status_code == 200:
            commit_hash = response.json()["commit_hash"]
            print(f"Commit {commit_hash} created")
        else:
            print("Failed to create commit")

    def should_ignore(self,file_path):
            """æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ¹é…å¿½ç•¥è§„åˆ™"""
            for pattern in IGNORE_PATTERNS:
                if pattern in file_path:
                    return True
            return False
    def push(self, remote, model_name, model_version):
        """æ‰“åŒ…æ–‡ä»¶å¹¶æ¨é€åˆ°æœåŠ¡ç«¯"""
        index = self.read_index()
        if not index:
            print("No files to push")
            return

        operations = index.get("operations", [])
        # åˆ›å»ºå†…å­˜ä¸­çš„ ZIP æ–‡ä»¶
        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zipf:
            for file_path, file_hash in index.items():
                if self.should_ignore(file_path):
                    print(f"Ignored {file_path}")
                    continue
                if file_path in ["operations","commit_data"]:  # è·³è¿‡æ“ä½œä¿¡æ¯
                    continue
                zipf.write(file_path, os.path.relpath(file_path, self.repo_path))
                print(f"Added to ZIP: {file_path}")

        # é‡ç½®ç¼“å†²åŒºæŒ‡é’ˆ
        zip_buffer.seek(0)

        # å‘é€ ZIP æ–‡ä»¶åˆ°æœåŠ¡ç«¯
        print(f"Pushing files to server: {list(index.keys())}")
        print(f"operations data: {operations}")
        self.load_session(server_name=self.server_name)
        response = self.session.post(
            f"{self.get_server_url()}/push",
            files={"file": (f"{model_name}_{model_version}.zip", zip_buffer)},
            data={
                "remote": remote,
                "model_name": model_name,
                "model_version": model_version,
                "remote": remote,
                "operations": json.dumps({"operations":operations}),
            },
        )
        print(f"Server response: {response.status_code}, {response.text}")
        if response.status_code == 200:
            print("Push completed")
        else:
            print("Push failed")

    def pull(self, remote, model_name, model_version):
        """ä»æœåŠ¡ç«¯æ‹‰å– ZIP æ–‡ä»¶å¹¶è§£å‹"""
        self.load_session(server_name=self.server_name)
        response = self.session.get(
            f"{self.get_server_url()}/pull",
            params={
                "model_name": model_name,
                "model_version": model_version,
                "remote": remote,
            },
        )
        if response.status_code == 200:
            # å°†å“åº”å†…å®¹å†™å…¥å†…å­˜ä¸­çš„ ZIP æ–‡ä»¶
            zip_buffer = io.BytesIO(response.content)
            with zipfile.ZipFile(zip_buffer, "r") as zipf:
                zipf.extractall(self.repo_path)
            print("Pull completed")
        else:
            print("Pull failed")


def init_command(args):
    """å¤„ç† init å‘½ä»¤"""
    file_name = args.file
    force = args.force

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if Path(file_name).exists() and not force:
        print(f"âš ï¸  æ–‡ä»¶ {file_name} å·²å­˜åœ¨ï¼Œä½¿ç”¨ --force è¦†ç›–")
        return

    # å†™å…¥æ–‡ä»¶
    with open(file_name, "w") as f:
        f.write(DEFAULT_ENV_TEMPLATE)

    # åˆ›å»ºç¤ºä¾‹ç›®å½•ç»“æ„
    Path("./models/dev").mkdir(parents=True, exist_ok=True)
    Path("./logs").mkdir(exist_ok=True)

    print(f"âœ… å·²ç”Ÿæˆé»˜è®¤ç¯å¢ƒæ–‡ä»¶: {file_name}")
    print("ğŸ‘‰ è¯·æ ¹æ®éœ€æ±‚ä¿®æ”¹ä»¥ä¸‹ç›®å½•é…ç½®ï¼š")
    print(f"   - å¼€å‘ç¯å¢ƒæ¨¡å‹ç›®å½•: ./models/dev")
    print(f"   - ç”Ÿäº§ç¯å¢ƒæ¨¡å‹ç›®å½•: ./models/prod")
    print(f"   - æ—¥å¿—ç›®å½•: {log_base_path}")


def create_directories(base_dir, src_dir, utils_dir):
    """åˆ›å»ºé¡¹ç›®æ‰€éœ€çš„ç›®å½•"""
    try:
        utils_dir.mkdir(parents=True, exist_ok=True)
        logging.info(f"æˆåŠŸåˆ›å»ºç›®å½•: {utils_dir}")
    except FileExistsError:
        logging.warning(f"ç›®å½• {utils_dir} å·²å­˜åœ¨")
    except PermissionError:
        logging.error(f"æ²¡æœ‰æƒé™åˆ›å»ºç›®å½•: {utils_dir}")
    except Exception as e:
        logging.error(f"åˆ›å»ºç›®å½• {utils_dir} æ—¶å‡ºç°é”™è¯¯: {e}")


def write_files(files):
    """å†™å…¥é¡¹ç›®æ‰€éœ€çš„æ–‡ä»¶"""
    for path, content in files.items():
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, "w") as f:
                f.write(content)
            logging.info(f"æˆåŠŸåˆ›å»ºæ–‡ä»¶: {path}")
        except PermissionError:
            logging.error(f"æ²¡æœ‰æƒé™åˆ›å»ºæ–‡ä»¶: {path}")
        except Exception as e:
            logging.error(f"åˆ›å»ºæ–‡ä»¶ {path} æ—¶å‡ºç°é”™è¯¯: {e}")


def create_gitignore(model_name):
    """åˆ›å»º .gitignore æ–‡ä»¶"""
    gitignore_path = Path(model_name) / ".gitignore"
    try:
        with open(gitignore_path, "w") as f:
            f.write("__pycache__/\n*.pyc\n*.pyo\n*.pyd\n")
        logging.info(f"æˆåŠŸåˆ›å»º .gitignore æ–‡ä»¶: {gitignore_path}")
    except PermissionError:
        logging.error(f"æ²¡æœ‰æƒé™åˆ›å»º .gitignore æ–‡ä»¶: {gitignore_path}")
    except Exception as e:
        logging.error(f"åˆ›å»º .gitignore æ–‡ä»¶æ—¶å‡ºç°é”™è¯¯: {e}")


def create_project(args):
    """ç”Ÿæˆç»Ÿä¸€çš„é¡¹ç›®ç»“æ„"""
    model_name = args.name
    version = args.version
    base_dir = Path(model_name) / version
    src_dir = base_dir / "src"
    utils_dir = src_dir / "utils"

    # åˆ›å»ºç›®å½•
    create_directories(base_dir, src_dir, utils_dir)

    # åˆ›å»ºæ ¸å¿ƒæ–‡ä»¶
    files = {
        base_dir / "config.yml": "# æ¨¡å‹é…ç½®\n",
        base_dir / "requirements.txt": "# é¡¹ç›®ä¾èµ–\n",
        base_dir / "README.md": f"# {model_name}\n\n## ç‰ˆæœ¬ {version}\n",
        src_dir / "__init__.py": "# æ ¸å¿ƒæ¨¡å‹æ¨¡å—\n",
        src_dir / "decision_module.py": "# å†³ç­–æ¨¡å—\n",
        src_dir / "strategy_module.py": "# ç­–ç•¥æ¨¡å—\n",
        src_dir
        / "model.py": f"""from optiflux.core import BaseModel
import logging

logger = logging.getLogger("optiflux.{model_name}")

class {model_name.title()}Model(BaseModel):
    DEFAULT_CONFIG = {{
        "model_path": "models/{model_name}_v1.pt",
        "threshold": 0.5
    }}

    def load(self):
        logger.info("Loading {model_name} model...")
        # åŠ è½½æ¨¡å‹é€»è¾‘

    def predict(self, input_data):
        logger.debug("Predicting with {model_name} model...")
        # é¢„æµ‹é€»è¾‘
""",
        src_dir
        / "recomserver.py": f"""from optiflux import BaseModel, ModelLibrary, make
from optiflux.utils.logx import log_recom_error, log_recom_debug
from optiflux import make

import traceback
import numpy as np
import os
import json

from utils import MODEL_ENV, VERSION


class RecomServer(BaseModel):
    def _load(self):
        self.model_name = f"{model_name}"
  
        self.model_db = make(
            f"cache/{{self.model_name}}-v{{VERSION}}",
            db_name="{model_name}.db",
            env=MODEL_ENV,
        )

    def _predict(self, items):
        uid = items.get("uid")
        request_id = items.get("request_id")
        try:
            print("business and model process")

            return items
        except:
            # å°†å¼‚å¸¸å †æ ˆä¿¡æ¯å†™å…¥é”™è¯¯æ—¥å¿—æ–‡ä»¶
            error_content = [
                f"{{self.model_name}}:{{request_id}}-error",
                str(traceback.format_exc()),
            ]
            log_recom_error([error_content])
            return items

# åˆå§‹åŒ–æ¨¡å‹åº“
library = ModelLibrary(
    models={{"recomserver": RecomServer}},
    #config_path="config.yml",
        #cache_dir=".prod_cache",
    size_limit=5*1024**3  # 5GB ç¼“å­˜
)

# åˆ›å»º API åº”ç”¨
api_config={{
        "title": "Production Recomserver API",
        "api_prefix": "",
        "enable_docs": True
    }}
api_service = create_optiflux_app(
    library,
    **api_config
)
app = api_service.app  # âœ… å…³é”®ï¼šå¯¼å‡º FastAPI å®ä¾‹
""",
        src_dir
        / "rewardserver.py": f"""from optiflux import BaseModel, ModelLibrary, make
from optiflux.utils.logx import log_reward_error, log_reward_debug
from optiflux.api import create_optiflux_app


import traceback
import numpy as np
import os
import json

from utils import MODEL_ENV, VERSION


class RewardServer(BaseModel):
    def _load(self):
        self.model_name = f"{model_name}"
  
        self.model_db = make(
            f"cache/{{self.model_name}}-v{{VERSION}}",
            db_name="{model_name}.db",
            env=MODEL_ENV,
        )

    def _predict(self, items):
        uid = items.get("uid")
        request_id = items.get("request_id")
        try:
            print("business and model process")

            return items
        except:
            # å°†å¼‚å¸¸å †æ ˆä¿¡æ¯å†™å…¥é”™è¯¯æ—¥å¿—æ–‡ä»¶
            error_content = [
                f"{{self.model_name}}:{{request_id}}-error",
                str(traceback.format_exc()),
            ]
            log_reward_error([error_content])
            return items

# åˆå§‹åŒ–æ¨¡å‹åº“
library = ModelLibrary(
    models={{"rewardserver": RewardServer}},
    #config_path="config.yml",
    #cache_dir=".prod_cache",
    size_limit=5*1024**3  # 5GB ç¼“å­˜
)

# åˆ›å»º API åº”ç”¨
api_config={{
        "title": "Production Rewardserver API",
        "api_prefix": "",
        "enable_docs": True
    }}
api_service = create_optiflux_app(
    library,
    **api_config
)
app = api_service.app  # âœ… å…³é”®ï¼šå¯¼å‡º FastAPI å®ä¾‹
""",
        utils_dir / "__init__.py": "# å·¥å…·æ¨¡å—\n",
        utils_dir
        / "config_loader.py": """import yaml
from pathlib import Path

def load_config(config_path: str) -> dict:
    with open(config_path, "r") as f:
        return yaml.safe_load(f)
""",
        utils_dir
        / "validation.py": """from typing import Any

def validate_input(data: Any) -> bool:
    return isinstance(data, (str, list, dict))
""",
    }

    # å†™å…¥æ–‡ä»¶
    write_files(files)

    # åˆ›å»º .gitignore
    create_gitignore(model_name)

    logging.info(f"é¡¹ç›® {model_name} å·²ç”Ÿæˆï¼Œç‰ˆæœ¬ {version}ã€‚")


def load_config():
    """åŠ è½½æœåŠ¡å™¨é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.getcwd(), "servers.yaml")
    if not os.path.exists(config_path):
        return {"servers": {}}
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def save_config(config):
    """ä¿å­˜æœåŠ¡å™¨é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.getcwd(), "servers.yaml")
    with open(config_path, "w") as f:
        yaml.safe_dump(config, f)


def config_list(args):
    """åˆ—å‡ºæ‰€æœ‰æœåŠ¡å™¨é…ç½®"""
    config = load_config()
    if not config["servers"]:
        print("No servers configured.")
        return
    for server_name, server_info in config["servers"].items():
        print(f"Server: {server_name}")
        for key, value in server_info.items():
            print(f"  {key}: {value}")


def config_add(args):
    """æ·»åŠ æ–°çš„æœåŠ¡å™¨é…ç½®"""
    config = load_config()
    if args.name in config["servers"]:
        print(f"Server '{args.name}' already exists. Use 'config update' to modify it.")
        return
    config["servers"][args.name] = {"url": args.url, "api_key": args.api_key}
    save_config(config)
    print(f"Server '{args.name}' added successfully.")


def config_update(args):
    """æ›´æ–°ç°æœ‰æœåŠ¡å™¨é…ç½®"""
    config = load_config()
    if args.name not in config["servers"]:
        print(f"Server '{args.name}' does not exist. Use 'config add' to create it.")
        return
    if args.url:
        config["servers"][args.name]["url"] = args.url
    if args.api_key:
        config["servers"][args.name]["api_key"] = args.api_key
    save_config(config)
    print(f"Server '{args.name}' updated successfully.")


def config_remove(args):
    """åˆ é™¤æœåŠ¡å™¨é…ç½®"""
    config = load_config()
    if args.name not in config["servers"]:
        print(f"Server '{args.name}' does not exist.")
        return
    del config["servers"][args.name]
    save_config(config)
    print(f"Server '{args.name}' removed successfully.")


def main():
    parser = argparse.ArgumentParser(description="Optiflux MLOps")
    subparsers = parser.add_subparsers(title="commands", dest="command")

    # init å­å‘½ä»¤
    init_parser = subparsers.add_parser("init", help="åˆå§‹åŒ–ç¯å¢ƒé…ç½®")
    init_parser.add_argument(
        "-f", "--file", default=".env", help="ç”Ÿæˆçš„ç¯å¢ƒæ–‡ä»¶å (é»˜è®¤: .env)"
    )
    init_parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨æ–‡ä»¶")
    init_parser.set_defaults(func=init_command)

    # åˆ›å»ºæ¨¡å‹é¡¹ç›® å­å‘½ä»¤
    create_parser = subparsers.add_parser("create-project", help="åˆ›å»ºæ¨¡å‹é¡¹ç›®")
    create_parser.add_argument(
        "--name", default="mymodel", required=True, help="æ¨¡å‹åç§°"
    )
    create_parser.add_argument("--version", default="0.0", help="ç‰ˆæœ¬å·")
    create_parser.set_defaults(func=create_project)

    # Add command
    add_parser = subparsers.add_parser("add", help="Add files to the staging area")
    add_parser.add_argument("path", help="Path to the file or directory")
    add_parser.set_defaults(
        func=lambda args: OptifluxClient(os.getcwd()).add(args.path)
    )

    # Commit command
    commit_parser = subparsers.add_parser("commit", help="Commit changes")
    commit_parser.add_argument("-m", "--message", required=True, help="Commit message")
    commit_parser.set_defaults(
        func=lambda args: OptifluxClient(os.getcwd()).commit(args.message)
    )

    # Push command
    push_parser = subparsers.add_parser("push", help="Push changes to remote")
    push_parser.add_argument("remote", help="Remote name")
    push_parser.add_argument("model_name", help="Model name")
    push_parser.add_argument("model_version", help="Model version")
    push_parser.add_argument("--server", help="Server name to use")
    push_parser.set_defaults(
        func=lambda args: OptifluxClient(os.getcwd(), args.server).push(
            args.remote, args.model_name, args.model_version
        )
    )

    # Pull command
    pull_parser = subparsers.add_parser("pull", help="Pull changes from remote")
    pull_parser.add_argument("remote", help="Remote name")
    pull_parser.add_argument("model_name", help="Model name")
    pull_parser.add_argument("model_version", help="Model version")
    pull_parser.add_argument("--server", help="Server name to use")
    pull_parser.set_defaults(
        func=lambda args: OptifluxClient(os.getcwd(), args.server).pull(
            args.remote, args.model_name, args.model_version
        )
    )

    # ç™»å½•å­å‘½ä»¤
    login_parser = subparsers.add_parser("login", help="Login to the server")
    login_parser.add_argument("username", help="Username")
    login_parser.add_argument("password", help="Password")
    login_parser.add_argument("--server", help="Server name to use")
    login_parser.set_defaults(
        func=lambda args: OptifluxClient(os.getcwd(), args.server).login(
            args.username, args.password,args.server
        )
    )

    # Config å­å‘½ä»¤
    config_parser = subparsers.add_parser("config", help="Manage server configurations")
    config_subparsers = config_parser.add_subparsers(
        title="config commands", dest="config_command"
    )

    # config list å­å‘½ä»¤
    config_list_parser = config_subparsers.add_parser(
        "list", help="List all server configurations"
    )
    config_list_parser.set_defaults(func=config_list)

    # config add å­å‘½ä»¤
    config_add_parser = config_subparsers.add_parser(
        "add", help="Add a new server configuration"
    )
    config_add_parser.add_argument("name", help="Server name")
    config_add_parser.add_argument("url", help="Server URL")
    config_add_parser.add_argument("api_key", help="Server API key")
    config_add_parser.set_defaults(func=config_add)

    # config update å­å‘½ä»¤
    config_update_parser = config_subparsers.add_parser(
        "update", help="Update an existing server configuration"
    )
    config_update_parser.add_argument("name", help="Server name")
    config_update_parser.add_argument("--url", help="New server URL")
    config_update_parser.add_argument("--api_key", help="New server API key")
    config_update_parser.set_defaults(func=config_update)

    # config remove å­å‘½ä»¤
    config_remove_parser = config_subparsers.add_parser(
        "remove", help="Remove a server configuration"
    )
    config_remove_parser.add_argument("name", help="Server name")
    config_remove_parser.set_defaults(func=config_remove)

    # Parse arguments and execute the corresponding function
    args = parser.parse_args()
    if hasattr(args, "func"):
        args.func(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
